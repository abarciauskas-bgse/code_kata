## Bootstrapping


## Confidence Intervals and their significance
## how over sampling works
## significance of the ROC curve and how to interpret a ROC curve
## How Random forest works
## Practical experience about Overfitting and Underfitting
## Practical experience about variable selection
## Basics of Logistic Regression

## What's the difference between MLE and MAP inference?

## How to deal with unbalance data where the ratio of positive and negative is huge.

## Estimate the disease probability in one city given the probability is very low national wide. Randomly asked 1000 person in this city, with all negative response(NO disease). What is the probability of disease in this city.

## i. Tell me about the supervised machine learning techniques that you know about?

## ii. If you have a customer and want to decide whether they will “buy today” or “not buy today” and you know 1. where they live, 2. their income, 3. their gender, 4. their profession, how would you define a machine learning algorithm.

## iii. How does a neural network with one layer and one input and output compare to a logistic regression.

## iv. For a long sorted list and a short (4 element) sorted list, what algorithm would you use to search the long list for the 4 elements.

## v. How would the algorithm above scale.

## vi. Given an unfair coin with the probability of heads not equal to .5. What algorithm could you use to create a list of random 1s and 0s.

## What is the curse of dimensionality

[Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)

## Why use L1 or L2 regularization?

# Regularization

[Answer from Justin Solomon on Quora](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization/answer/Justin-Solomon)

Can be viewed as:

* putting a prior on the distribution from which your data was drawn,
* as a way to punish high values in regression coefficients

*undetermined:* **`Ax = b`** is undertermined when A is wider than it is tall, thus there exists infinitely many solutions. (Multiple possible `x`'s)

Thus we chose to minimize ||x|| wrt Ax = b



